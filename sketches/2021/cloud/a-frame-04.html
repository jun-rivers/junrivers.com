<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Image based tracking AR.js demo</title>
    <!-- import aframe and then ar.js with image tracking / location based features -->
    <script src="https://aframe.io/releases/1.0.4/aframe.min.js"></script>
    <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

    <!-- style for the loader -->
    <style>
      .arjs-loader {
        height: 100%;
        width: 100%;
        position: absolute;
        top: 0;
        left: 0;
        background-color: rgba(0, 0, 0, 0.8);
        z-index: 9999;
        display: flex;
        justify-content: center;
        align-items: center;
      }

      .arjs-loader div {
        text-align: center;
        font-size: 1.25em;
        color: white;
      }

      .circularVision {
        position: fixed;
        width: 80vw;
        height: 80vw;
        border-radius: 40vw;
        background: none;
        border: 4px solid white;
        top: calc(50vh - 40vw);
        left: 10vw;
        opacity: 0.5;
      }
      .circularVisionProgress {
        width: 80vw;
        height: 80vw;
        border-radius: 40vw;
        background: none;
        border: 4px solid white;
        top: calc(50vh - 40vw);
        left: 10vw;
        opacity: 1;
      }

      .circularVision .circularVisionProgress .mask,
      .circularVision .circularVisionProgress .fill {
        width: 150px;
        height: 150px;
        position: absolute;
        border-radius: 50%;
      }

      .mask .fill {
        clip: rect(0px, 75px, 150px, 0px);
        background-color: #227ded;
      }

      .mask.full,
      .circle .fill {
        animation: fill ease-in-out 3s;
        transform: rotate(135deg);
      }

      @keyframes fill {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(135deg);
        }
      }
    </style>
    <script>
      var observed, observationFrames
      observationFrames = 0;
      let observing = false;

      window.addEventListener("markerFound", (event) => {
        console.log('markerFound');
        observing = true;
      });

      window.addEventListener("markerLost", (event) => {
        console.log('markerLost');
        observing = false;
      });
      
      window.onload = function() {    
        function observationCheck() {
          if (observing) {
            observationFrames++;
          }
          if (observationFrames > 500) {
            console.log('observation done.');
            window.close();
          }
        }
        setInterval(observationCheck, 1);
       }

       // Tasks:
       // loading consistency
       // take in variables from urlparam: modelUrl, game id, step id, patternUrl, scaleInteger, observationFrames;
       // send 'done' message to parent;
       // Progress game after sending back
       // Location example;
      
    </script>
  </head>

  <body style="margin : 0px; overflow: hidden;">
    <!-- minimal loader shown until image descriptors are loaded. Loading may take a while according to the device computational power -->
    <div class="arjs-loader">
      <div>Loading, please wait...</div>
    </div>

    <div class="circularVision mask">
      <div class="circularVisionProgress">
        <div class="mask half">
            <div class="fill"></div>
        </div>
      </div>
    </div>

    <!-- a-frame scene -->
    <a-scene
      emitevents="true";
      vr-mode-ui="enabled: false;"
      renderer="logarithmicDepthBuffer: true;"
      embedded
      arjs="trackingMethod: best; sourceType: webcam;debugUIEnabled: false;">
      <!-- a-nft is the anchor that defines an Image Tracking entity -->
      <!-- on 'url' use the path to the Image Descriptors created before. -->
      <!-- the path should end with the name without the extension e.g. if file is trex.fset' the path should end with trex -->
      <a-nft
        type="nft"
        url="https://storage.googleapis.com/artifacts-cms/ad8xs-0hh6n"
        smooth="true"
        smoothCount="10"
        smoothTolerance=".01"
        smoothThreshold="5">
          <!-- as a child of the a-nft entity, you can define the content to show. here's a GLTF model entity -->
          <!-- gltf-model="./data/Flamingo.glb" -->
          <a-entity
            gltf-model="https://storage.googleapis.com/artifacts-cms/Flamingo.glb"
            scale="1.5 1.5 1.5"
            position="0 0 0"
            rotation="-90 180 180"
          >
          </a-entity>
      </a-nft>
      <!-- static camera that moves according to the device movemenents -->
      <a-entity camera></a-entity>
    </a-scene>
  </body>
</html>